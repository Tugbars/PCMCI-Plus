{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended PCMCI+ Analysis\n",
    "\n",
    "Deeper exploration:\n",
    "1. Different alpha levels (catch weaker signals)\n",
    "2. Different lag windows\n",
    "3. CMI vs ParCorr (nonlinear detection)\n",
    "4. Rolling window analysis (regime changes)\n",
    "5. Crisis period deep-dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import pcmci\n",
    "\n",
    "print(f\"PCMCI+ version: {pcmci.version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data\n",
    "ASSETS = {'SPY': 'US Equities', 'TLT': 'Treasuries', 'GLD': 'Gold', 'UUP': 'US Dollar', 'EEM': 'EM Equities'}\n",
    "\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=2*365)\n",
    "\n",
    "data = yf.download(list(ASSETS.keys()), start=start_date, end=end_date, progress=False)\n",
    "\n",
    "def parkinson_volatility(high, low, window=20):\n",
    "    log_hl = np.log(high / low) ** 2\n",
    "    return np.sqrt((1.0 / (4.0 * np.log(2))) * log_hl.rolling(window).mean() * 252)\n",
    "\n",
    "volatility = pd.DataFrame()\n",
    "for symbol in ASSETS.keys():\n",
    "    volatility[symbol] = parkinson_volatility(data['High'][symbol], data['Low'][symbol], 20)\n",
    "volatility = volatility.dropna()\n",
    "\n",
    "var_names = list(ASSETS.keys())\n",
    "vol_matrix = volatility[var_names].values.T\n",
    "vol_arr = volatility.values\n",
    "\n",
    "print(f\"Data: {vol_matrix.shape[1]} days, {vol_matrix.shape[0]} assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Different Alpha Levels\n",
    "\n",
    "Relaxing alpha catches weaker but potentially real signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross-Asset Spillovers at Different Significance Levels\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for alpha in [0.01, 0.05, 0.10, 0.20]:\n",
    "    result = pcmci.run_pcmci(vol_matrix, tau_max=5, alpha=alpha, var_names=var_names)\n",
    "    \n",
    "    # Cross-asset links only\n",
    "    cross = [l for l in result.significant_links if l.source_var != l.target_var]\n",
    "    \n",
    "    print(f\"\\nα = {alpha:.2f}: {len(cross)} cross-asset spillovers\")\n",
    "    for link in cross:\n",
    "        src, tgt = var_names[link.source_var], var_names[link.target_var]\n",
    "        print(f\"  {src}(t-{link.tau}) → {tgt}(t): r={link.val:+.3f}, p={link.pval:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Different Lag Windows\n",
    "\n",
    "Longer lags may reveal slower transmission channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross-Asset Spillovers at Different Lag Windows\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for tau_max in [3, 5, 10, 15, 20]:\n",
    "    result = pcmci.run_pcmci(vol_matrix, tau_max=tau_max, alpha=0.10, var_names=var_names)\n",
    "    \n",
    "    cross = [l for l in result.significant_links if l.source_var != l.target_var]\n",
    "    \n",
    "    print(f\"\\nτ_max = {tau_max:2d}: {len(cross)} links ({result.runtime*1000:.1f}ms)\")\n",
    "    for link in sorted(cross, key=lambda x: abs(x.val), reverse=True)[:5]:\n",
    "        src, tgt = var_names[link.source_var], var_names[link.target_var]\n",
    "        print(f\"  {src}(t-{link.tau:2d}) → {tgt}(t): r={link.val:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear vs Nonlinear Dependencies\n",
    "\n",
    "CMI can detect relationships that partial correlation misses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pairwise Comparison: Partial Correlation vs CMI\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Pair':<12} {'ParCorr':>9} {'p':>8} {'CMI':>9} {'p':>8}  Notes\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in range(len(var_names)):\n",
    "    for j in range(i+1, len(var_names)):\n",
    "        X, Y = vol_arr[:, i], vol_arr[:, j]\n",
    "        \n",
    "        r, p_r = pcmci.parcorr_test(X, Y)\n",
    "        cmi_result = pcmci.cmi_test(X, Y, n_perm=100)\n",
    "        \n",
    "        pair = f\"{var_names[i]}-{var_names[j]}\"\n",
    "        \n",
    "        # Flag interesting cases\n",
    "        note = \"\"\n",
    "        if p_r > 0.05 and cmi_result.pvalue < 0.05:\n",
    "            note = \"← NONLINEAR!\"\n",
    "        elif abs(r) > 0.5:\n",
    "            note = \"← Strong\"\n",
    "        \n",
    "        print(f\"{pair:<12} {r:>+9.3f} {p_r:>8.4f} {cmi_result.cmi:>9.3f} {cmi_result.pvalue:>8.4f}  {note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lead-Lag Deep Dive\n",
    "\n",
    "Detailed lag-by-lag analysis for key pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lead_lag_plot(source_idx, target_idx, max_lag=15):\n",
    "    \"\"\"Analyze and plot lead-lag relationship\"\"\"\n",
    "    src_name, tgt_name = var_names[source_idx], var_names[target_idx]\n",
    "    \n",
    "    lags = list(range(1, max_lag + 1))\n",
    "    parcorrs = []\n",
    "    pvalues = []\n",
    "    cmis = []\n",
    "    \n",
    "    for lag in lags:\n",
    "        source = vol_arr[:-lag, source_idx]\n",
    "        target = vol_arr[lag:, target_idx]\n",
    "        \n",
    "        r, p = pcmci.parcorr_test(source, target)\n",
    "        mi = pcmci.mi(source, target)\n",
    "        \n",
    "        parcorrs.append(r)\n",
    "        pvalues.append(p)\n",
    "        cmis.append(mi)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Partial correlation\n",
    "    colors = ['green' if p < 0.05 else 'lightgray' for p in pvalues]\n",
    "    axes[0].bar(lags, parcorrs, color=colors, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "    axes[0].set_xlabel('Lag (days)')\n",
    "    axes[0].set_ylabel('Partial Correlation')\n",
    "    axes[0].set_title(f'{src_name}(t-lag) → {tgt_name}(t)\\nGreen = p < 0.05')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # CMI\n",
    "    axes[1].bar(lags, cmis, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_xlabel('Lag (days)')\n",
    "    axes[1].set_ylabel('Mutual Information (nats)')\n",
    "    axes[1].set_title(f'{src_name}(t-lag) → {tgt_name}(t)\\nMI (includes nonlinear)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best lags\n",
    "    best_parcorr_idx = np.argmax(np.abs(parcorrs))\n",
    "    best_cmi_idx = np.argmax(cmis)\n",
    "    \n",
    "    print(f\"Best ParCorr: lag={lags[best_parcorr_idx]}, r={parcorrs[best_parcorr_idx]:+.3f}, p={pvalues[best_parcorr_idx]:.4f}\")\n",
    "    print(f\"Best CMI:     lag={lags[best_cmi_idx]}, MI={cmis[best_cmi_idx]:.3f}\")\n",
    "\n",
    "# Analyze key pairs\n",
    "print(\"TLT → SPY (Do rates lead equities?)\")\n",
    "lead_lag_plot(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SPY → EEM (Do US equities lead EM?)\")\n",
    "lead_lag_plot(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TLT → UUP (Do bonds lead dollar?)\")\n",
    "lead_lag_plot(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GLD → SPY (Does gold lead equities?)\")\n",
    "lead_lag_plot(2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rolling Window Analysis (Time-Varying Causality)\n",
    "\n",
    "Causal relationships change over time. Let's track them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 120  # ~6 months\n",
    "step = 10  # ~2 weeks\n",
    "\n",
    "# Track multiple relationships\n",
    "results_over_time = {f\"{s}->{t}\": [] for s in var_names for t in var_names if s != t}\n",
    "dates = []\n",
    "\n",
    "print(f\"Running rolling PCMCI+ (window={window_size}, step={step})...\")\n",
    "\n",
    "for start in range(0, vol_matrix.shape[1] - window_size, step):\n",
    "    end = start + window_size\n",
    "    window_data = vol_matrix[:, start:end]\n",
    "    window_date = volatility.index[end - 1]\n",
    "    \n",
    "    result = pcmci.run_pcmci(window_data, tau_max=3, alpha=0.20, var_names=var_names)\n",
    "    \n",
    "    # Extract all cross-asset links\n",
    "    link_dict = {}\n",
    "    for link in result.significant_links:\n",
    "        if link.source_var != link.target_var:\n",
    "            key = f\"{var_names[link.source_var]}->{var_names[link.target_var]}\"\n",
    "            link_dict[key] = link.val\n",
    "    \n",
    "    for key in results_over_time:\n",
    "        results_over_time[key].append(link_dict.get(key, 0))\n",
    "    \n",
    "    dates.append(window_date)\n",
    "\n",
    "print(f\"Completed {len(dates)} windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot key relationships over time\n",
    "key_pairs = ['SPY->EEM', 'TLT->SPY', 'TLT->UUP', 'GLD->SPY']\n",
    "\n",
    "fig, axes = plt.subplots(len(key_pairs), 1, figsize=(14, 3*len(key_pairs)), sharex=True)\n",
    "\n",
    "for ax, pair in zip(axes, key_pairs):\n",
    "    values = results_over_time[pair]\n",
    "    \n",
    "    ax.plot(dates, values, 'b-', linewidth=1.5)\n",
    "    ax.fill_between(dates, values, 0, alpha=0.3, \n",
    "                    color=['green' if v > 0 else 'red' for v in values])\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax.set_ylabel('Strength')\n",
    "    ax.set_title(f'{pair} (Rolling 6-month PCMCI+)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. April 2025 Crisis Deep-Dive\n",
    "\n",
    "Analyze the volatility spike in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find crisis period\n",
    "crisis_mask = volatility['SPY'] > 0.25\n",
    "\n",
    "if crisis_mask.any():\n",
    "    crisis_start = volatility.index[crisis_mask].min()\n",
    "    crisis_end = volatility.index[crisis_mask].max()\n",
    "    peak_date = volatility['SPY'].idxmax()\n",
    "    \n",
    "    print(f\"Crisis period: {crisis_start.date()} to {crisis_end.date()}\")\n",
    "    print(f\"Peak date: {peak_date.date()} (SPY vol = {volatility.loc[peak_date, 'SPY']:.1%})\")\n",
    "    \n",
    "    # Show day-by-day around peak\n",
    "    print(\"\\nDay-by-day volatility around peak:\")\n",
    "    window = volatility.loc[peak_date - timedelta(days=10):peak_date + timedelta(days=5)]\n",
    "    \n",
    "    # Format nicely\n",
    "    display_df = (window * 100).round(1)  # Convert to percentage\n",
    "    display_df.columns = [f\"{c} (%)\" for c in display_df.columns]\n",
    "    display(display_df)\n",
    "else:\n",
    "    print(\"No crisis period found (SPY vol never exceeded 25%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCMCI+ specifically on crisis period\n",
    "if crisis_mask.any():\n",
    "    # Include 60 days before crisis for lead detection\n",
    "    pre_crisis_start = crisis_start - timedelta(days=60)\n",
    "    crisis_data = volatility.loc[pre_crisis_start:crisis_end]\n",
    "    \n",
    "    print(f\"Crisis analysis window: {crisis_data.index[0].date()} to {crisis_data.index[-1].date()}\")\n",
    "    print(f\"Observations: {len(crisis_data)}\\n\")\n",
    "    \n",
    "    crisis_matrix = crisis_data[var_names].values.T\n",
    "    \n",
    "    result = pcmci.run_pcmci(crisis_matrix, tau_max=5, alpha=0.10, var_names=var_names)\n",
    "    \n",
    "    print(\"Causal links during crisis build-up:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    cross_links = [l for l in result.significant_links if l.source_var != l.target_var]\n",
    "    for link in sorted(cross_links, key=lambda x: abs(x.val), reverse=True):\n",
    "        src, tgt = var_names[link.source_var], var_names[link.target_var]\n",
    "        print(f\"  {src}(t-{link.tau}) → {tgt}(t): r={link.val:+.3f}, p={link.pval:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distance Correlation Heatmap\n",
    "\n",
    "Detect any dependence (including nonlinear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = len(var_names)\n",
    "dcor_matrix = np.eye(n_assets)\n",
    "\n",
    "for i in range(n_assets):\n",
    "    for j in range(i+1, n_assets):\n",
    "        dc = pcmci.dcor(vol_arr[:, i], vol_arr[:, j])\n",
    "        dcor_matrix[i, j] = dc\n",
    "        dcor_matrix[j, i] = dc\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(dcor_matrix, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(n_assets))\n",
    "ax.set_yticks(range(n_assets))\n",
    "ax.set_xticklabels(var_names, fontsize=12)\n",
    "ax.set_yticklabels(var_names, fontsize=12)\n",
    "\n",
    "for i in range(n_assets):\n",
    "    for j in range(n_assets):\n",
    "        ax.text(j, i, f'{dcor_matrix[i,j]:.2f}', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_title('Distance Correlation Matrix\\n(Detects Any Dependence)', fontsize=14)\n",
    "plt.colorbar(im, label='dCor')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Finding | Tradeable? | Action |\n",
    "|---------|------------|--------|\n",
    "| SPY ↔ EEM contemporaneous | ❌ | No lead time |\n",
    "| TLT → UUP at lag 5 | ✅ | Watch bond vol for dollar vol |\n",
    "| GLD independent | ✅ | True diversifier |\n",
    "| Strong AR(1) everywhere | ✅ | Vol clustering = predictable |\n",
    "\n",
    "### Next Steps\n",
    "1. Add more assets (VIX, BTC, sector ETFs)\n",
    "2. Try intraday data for faster signals\n",
    "3. Integrate with your BOCPD for regime detection\n",
    "4. Build alerting system for spillover detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
