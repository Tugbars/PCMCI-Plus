{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCMCI+ Causal Discovery with Real Market Data\n",
    "\n",
    "This notebook demonstrates how to use the high-performance PCMCI+ C library to discover causal relationships in financial time series.\n",
    "\n",
    "**Features demonstrated:**\n",
    "- Fetching market data with yfinance\n",
    "- Volatility estimation (Parkinson, returns)\n",
    "- PCMCI+ causal graph discovery\n",
    "- CMI (Conditional Mutual Information) for nonlinear dependencies\n",
    "- Distance Correlation for any-dependence detection\n",
    "- Visualizing causal graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install yfinance pandas numpy matplotlib networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import PCMCI+ library\n",
    "import pcmci\n",
    "\n",
    "print(f\"PCMCI+ version: {pcmci.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch Market Data\n",
    "\n",
    "We'll analyze cross-market relationships between:\n",
    "- **SPY** - S&P 500 (US Equities)\n",
    "- **TLT** - 20+ Year Treasury Bonds (Rates)\n",
    "- **GLD** - Gold (Safe haven)\n",
    "- **UUP** - US Dollar Index (Currency)\n",
    "- **EEM** - Emerging Markets (Risk sentiment)\n",
    "- **VIX** - Volatility Index (Fear gauge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define assets to analyze\n",
    "ASSETS = {\n",
    "    'SPY': 'US Equities',\n",
    "    'TLT': 'Treasuries',\n",
    "    'GLD': 'Gold',\n",
    "    'UUP': 'US Dollar',\n",
    "    'EEM': 'EM Equities',\n",
    "}\n",
    "\n",
    "# Fetch 2 years of daily data\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=2*365)\n",
    "\n",
    "print(f\"Fetching data from {start_date.date()} to {end_date.date()}...\")\n",
    "\n",
    "data = yf.download(\n",
    "    list(ASSETS.keys()),\n",
    "    start=start_date,\n",
    "    end=end_date,\n",
    "    progress=False\n",
    ")\n",
    "\n",
    "print(f\"Downloaded {len(data)} days of data\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Volatility\n",
    "\n",
    "We'll use two volatility measures:\n",
    "1. **Parkinson volatility** - High-low range estimator (more efficient than close-to-close)\n",
    "2. **Realized volatility** - Rolling std of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parkinson_volatility(high, low, window=20):\n",
    "    \"\"\"Parkinson high-low volatility estimator (annualized)\"\"\"\n",
    "    log_hl = np.log(high / low) ** 2\n",
    "    factor = 1.0 / (4.0 * np.log(2))\n",
    "    vol = np.sqrt(factor * log_hl.rolling(window).mean() * 252)\n",
    "    return vol\n",
    "\n",
    "def realized_volatility(close, window=20):\n",
    "    \"\"\"Rolling standard deviation of log returns (annualized)\"\"\"\n",
    "    returns = np.log(close / close.shift(1))\n",
    "    vol = returns.rolling(window).std() * np.sqrt(252)\n",
    "    return vol\n",
    "\n",
    "# Compute volatility for each asset\n",
    "vol_window = 20\n",
    "volatility = pd.DataFrame()\n",
    "\n",
    "for symbol in ASSETS.keys():\n",
    "    high = data['High'][symbol]\n",
    "    low = data['Low'][symbol]\n",
    "    close = data['Close'][symbol]\n",
    "    \n",
    "    # Use Parkinson volatility\n",
    "    volatility[symbol] = parkinson_volatility(high, low, vol_window)\n",
    "\n",
    "# Drop NaN rows\n",
    "volatility = volatility.dropna()\n",
    "print(f\"Volatility series: {len(volatility)} observations\")\n",
    "\n",
    "volatility.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volatility time series\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for symbol in ASSETS.keys():\n",
    "    ax.plot(volatility.index, volatility[symbol], label=f\"{symbol} ({ASSETS[symbol]})\", alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Annualized Volatility')\n",
    "ax.set_title('Parkinson Volatility (20-day rolling)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run PCMCI+ Causal Discovery\n",
    "\n",
    "PCMCI+ will find:\n",
    "- Which assets' volatility **causes** changes in other assets' volatility\n",
    "- The **lag** at which this happens (e.g., TLT leads SPY by 2 days)\n",
    "- The **strength** of the causal link (partial correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PCMCI+\n",
    "# Shape must be (n_vars, T)\n",
    "var_names = list(ASSETS.keys())\n",
    "vol_matrix = volatility[var_names].values.T  # Shape: (5, T)\n",
    "\n",
    "print(f\"Data shape: {vol_matrix.shape} (n_vars={vol_matrix.shape[0]}, T={vol_matrix.shape[1]})\")\n",
    "\n",
    "# Run PCMCI+\n",
    "tau_max = 5  # Look for causal links up to 5 days lag\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "print(f\"\\nRunning PCMCI+ with tau_max={tau_max}, alpha={alpha}...\")\n",
    "\n",
    "result = pcmci.run_pcmci(\n",
    "    vol_matrix,\n",
    "    tau_max=tau_max,\n",
    "    alpha=alpha,\n",
    "    var_names=var_names,\n",
    "    use_spearman=True,  # Robust to outliers\n",
    "    n_threads=0  # Auto-detect\n",
    ")\n",
    "\n",
    "print(f\"\\nCompleted in {result.runtime:.3f} seconds\")\n",
    "print(f\"Found {result.n_significant} significant causal links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaner summary table\n",
    "links_data = []\n",
    "for link in result.significant_links:\n",
    "    src = var_names[link.source_var]\n",
    "    tgt = var_names[link.target_var]\n",
    "    links_data.append({\n",
    "        'Source': src,\n",
    "        'Target': tgt,\n",
    "        'Lag (days)': link.tau,\n",
    "        'Strength': f\"{link.val:.3f}\",\n",
    "        'p-value': f\"{link.pval:.4f}\",\n",
    "        'Interpretation': f\"{src} vol → {tgt} vol in {link.tau}d\"\n",
    "    })\n",
    "\n",
    "links_df = pd.DataFrame(links_data)\n",
    "if len(links_df) > 0:\n",
    "    # Sort by absolute strength\n",
    "    links_df['abs_strength'] = links_df['Strength'].astype(float).abs()\n",
    "    links_df = links_df.sort_values('abs_strength', ascending=False).drop('abs_strength', axis=1)\n",
    "    display(links_df)\n",
    "else:\n",
    "    print(\"No significant causal links found at alpha=0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Causal Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def plot_causal_graph(result, var_names, min_strength=0.0):\n",
    "    \"\"\"Plot causal graph using networkx\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for name in var_names:\n",
    "        G.add_node(name)\n",
    "    \n",
    "    # Add edges\n",
    "    edge_labels = {}\n",
    "    edge_colors = []\n",
    "    edge_widths = []\n",
    "    \n",
    "    for link in result.significant_links:\n",
    "        if abs(link.val) < min_strength:\n",
    "            continue\n",
    "        if link.tau == 0 and link.source_var == link.target_var:\n",
    "            continue  # Skip self-loops at lag 0\n",
    "            \n",
    "        src = var_names[link.source_var]\n",
    "        tgt = var_names[link.target_var]\n",
    "        \n",
    "        G.add_edge(src, tgt)\n",
    "        edge_labels[(src, tgt)] = f\"τ={link.tau}\\nr={link.val:.2f}\"\n",
    "        edge_colors.append('green' if link.val > 0 else 'red')\n",
    "        edge_widths.append(abs(link.val) * 5)\n",
    "    \n",
    "    if len(G.edges()) == 0:\n",
    "        print(\"No edges to display\")\n",
    "        return\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=3000, node_color='lightblue', \n",
    "                          edgecolors='black', linewidths=2, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=12, font_weight='bold', ax=ax)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths,\n",
    "                          arrows=True, arrowsize=25, arrowstyle='-|>',\n",
    "                          connectionstyle='arc3,rad=0.1', ax=ax)\n",
    "    \n",
    "    # Edge labels\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=9, ax=ax)\n",
    "    \n",
    "    ax.set_title('Volatility Spillover Graph (PCMCI+)\\nGreen=positive, Red=negative', fontsize=14)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_causal_graph(result, var_names, min_strength=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Independence Tests\n",
    "\n",
    "Let's compare different tests on pairs of assets:\n",
    "- **Partial Correlation** - Fast, linear only\n",
    "- **CMI** - Detects nonlinear dependencies\n",
    "- **Distance Correlation** - Detects any dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare independence tests on asset pairs\n",
    "def compare_tests(X, Y, name_x, name_y):\n",
    "    \"\"\"Compare different independence tests\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {name_x} vs {name_y}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Partial correlation\n",
    "    r, p = pcmci.parcorr_test(X, Y)\n",
    "    print(f\"Partial Correlation: r={r:+.4f}, p={p:.4f}\")\n",
    "    \n",
    "    # CMI\n",
    "    result = pcmci.cmi_test(X, Y, n_perm=100)\n",
    "    print(f\"CMI (KSG k=5):       cmi={result.cmi:.4f}, p={result.pvalue:.4f}\")\n",
    "    \n",
    "    # Distance correlation\n",
    "    result = pcmci.dcor_test(X, Y, n_perm=100)\n",
    "    print(f\"Distance Corr:       dcor={result.dcor:.4f}, p={result.pvalue:.4f}\")\n",
    "\n",
    "# Test a few pairs\n",
    "vol_arr = volatility.values\n",
    "\n",
    "compare_tests(vol_arr[:, 0], vol_arr[:, 1], 'SPY', 'TLT')\n",
    "compare_tests(vol_arr[:, 0], vol_arr[:, 2], 'SPY', 'GLD')\n",
    "compare_tests(vol_arr[:, 0], vol_arr[:, 4], 'SPY', 'EEM')\n",
    "compare_tests(vol_arr[:, 1], vol_arr[:, 2], 'TLT', 'GLD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conditional Independence Test\n",
    "\n",
    "Test if two assets are independent **after controlling for** a third asset.\n",
    "\n",
    "Example: Are SPY and EEM independent given GLD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_test(X, Y, Z, name_x, name_y, name_z):\n",
    "    \"\"\"Test X ⊥ Y | Z\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {name_x} ⊥ {name_y} | {name_z}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Unconditional first\n",
    "    r, p = pcmci.parcorr_test(X, Y)\n",
    "    print(f\"Unconditional parcorr({name_x}, {name_y}): r={r:+.4f}, p={p:.4f}\")\n",
    "    \n",
    "    # Conditional\n",
    "    r, p = pcmci.parcorr_test(X, Y, Z)\n",
    "    print(f\"Conditional parcorr({name_x}, {name_y} | {name_z}): r={r:+.4f}, p={p:.4f}\")\n",
    "    \n",
    "    # CMI conditional\n",
    "    result_uncond = pcmci.cmi_test(X, Y, n_perm=100)\n",
    "    result_cond = pcmci.cmi_test(X, Y, Z, n_perm=100)\n",
    "    print(f\"\\nCMI({name_x}; {name_y}):       {result_uncond.cmi:.4f}, p={result_uncond.pvalue:.4f}\")\n",
    "    print(f\"CMI({name_x}; {name_y} | {name_z}): {result_cond.cmi:.4f}, p={result_cond.pvalue:.4f}\")\n",
    "\n",
    "# Test conditional independence\n",
    "spy = vol_arr[:, 0]\n",
    "tlt = vol_arr[:, 1]\n",
    "gld = vol_arr[:, 2]\n",
    "uup = vol_arr[:, 3]\n",
    "eem = vol_arr[:, 4]\n",
    "\n",
    "conditional_test(spy, eem, gld, 'SPY', 'EEM', 'GLD')\n",
    "conditional_test(spy, gld, tlt, 'SPY', 'GLD', 'TLT')\n",
    "conditional_test(tlt, gld, uup, 'TLT', 'GLD', 'UUP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lead-Lag Analysis\n",
    "\n",
    "Which asset's volatility **leads** which? This is the core question for spillover detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lead_lag_analysis(source, target, name_src, name_tgt, max_lag=10):\n",
    "    \"\"\"Analyze lead-lag relationship between two assets\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for lag in range(0, max_lag + 1):\n",
    "        if lag == 0:\n",
    "            x = source\n",
    "            y = target\n",
    "        else:\n",
    "            x = source[:-lag]  # Source at t-lag\n",
    "            y = target[lag:]   # Target at t\n",
    "        \n",
    "        r, p = pcmci.parcorr_test(x, y)\n",
    "        results.append({'lag': lag, 'corr': r, 'pvalue': p})\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    colors = ['green' if p < 0.05 else 'gray' for p in df['pvalue']]\n",
    "    ax.bar(df['lag'], df['corr'], color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.set_xlabel('Lag (days)')\n",
    "    ax.set_ylabel('Correlation')\n",
    "    ax.set_title(f'Lead-Lag: {name_src}(t-lag) → {name_tgt}(t)\\nGreen = significant (p<0.05)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal lag\n",
    "    sig_results = df[df['pvalue'] < 0.05]\n",
    "    if len(sig_results) > 0:\n",
    "        best = sig_results.loc[sig_results['corr'].abs().idxmax()]\n",
    "        print(f\"Strongest link: {name_src}(t-{int(best['lag'])}) → {name_tgt}(t), r={best['corr']:.3f}\")\n",
    "    else:\n",
    "        print(\"No significant lead-lag relationship found\")\n",
    "\n",
    "# Analyze lead-lag for interesting pairs\n",
    "lead_lag_analysis(tlt, spy, 'TLT', 'SPY')\n",
    "lead_lag_analysis(spy, eem, 'SPY', 'EEM')\n",
    "lead_lag_analysis(uup, gld, 'UUP', 'GLD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Returns Analysis (Alternative to Volatility)\n",
    "\n",
    "We can also analyze causal relationships in returns instead of volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log returns\n",
    "returns = pd.DataFrame()\n",
    "for symbol in ASSETS.keys():\n",
    "    close = data['Close'][symbol]\n",
    "    returns[symbol] = np.log(close / close.shift(1))\n",
    "\n",
    "returns = returns.dropna()\n",
    "print(f\"Returns: {len(returns)} observations\")\n",
    "\n",
    "# Run PCMCI+ on returns\n",
    "returns_matrix = returns.values.T\n",
    "\n",
    "print(f\"\\nRunning PCMCI+ on returns...\")\n",
    "result_returns = pcmci.run_pcmci(\n",
    "    returns_matrix,\n",
    "    tau_max=5,\n",
    "    alpha=0.05,\n",
    "    var_names=var_names,\n",
    "    use_spearman=True\n",
    ")\n",
    "\n",
    "print(f\"Completed in {result_returns.runtime:.3f} seconds\")\n",
    "print(f\"Found {result_returns.n_significant} significant causal links in returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare volatility vs returns causal structures\n",
    "print(\"=\" * 60)\n",
    "print(\"VOLATILITY Causal Links:\")\n",
    "print(\"=\" * 60)\n",
    "for link in result.significant_links:\n",
    "    if link.tau > 0 or link.source_var != link.target_var:\n",
    "        src = var_names[link.source_var]\n",
    "        tgt = var_names[link.target_var]\n",
    "        print(f\"  {src}(t-{link.tau}) → {tgt}(t): r={link.val:+.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RETURNS Causal Links:\")\n",
    "print(\"=\" * 60)\n",
    "for link in result_returns.significant_links:\n",
    "    if link.tau > 0 or link.source_var != link.target_var:\n",
    "        src = var_names[link.source_var]\n",
    "        tgt = var_names[link.target_var]\n",
    "        print(f\"  {src}(t-{link.tau}) → {tgt}(t): r={link.val:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Performance Benchmark\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with different data sizes\n",
    "n_vars = 5\n",
    "for T in [100, 250, 500, 1000]:\n",
    "    data_test = np.random.randn(n_vars, T)\n",
    "    \n",
    "    start = time.time()\n",
    "    result = pcmci.run_pcmci(data_test, tau_max=3, alpha=0.05)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"n_vars={n_vars}, T={T:4d}: {elapsed*1000:6.1f} ms\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test independence tests\n",
    "X = np.random.randn(1000)\n",
    "Y = np.random.randn(1000)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    pcmci.parcorr_test(X, Y)\n",
    "print(f\"parcorr_test (1000 samples, 100 runs): {(time.time()-start)*10:.2f} ms/call\")\n",
    "\n",
    "start = time.time()\n",
    "pcmci.mi(X, Y)\n",
    "print(f\"MI (1000 samples): {(time.time()-start)*1000:.2f} ms\")\n",
    "\n",
    "start = time.time()\n",
    "pcmci.dcor(X, Y)\n",
    "print(f\"dCor (1000 samples): {(time.time()-start)*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### What we learned:\n",
    "\n",
    "1. **PCMCI+** discovers causal relationships with time lags in multivariate time series\n",
    "2. **Lead-lag detection**: Identifies which asset moves first and predicts others\n",
    "3. **Multiple tests available**:\n",
    "   - `parcorr_test`: Fast, linear relationships\n",
    "   - `cmi_test`: Nonlinear dependencies (KSG estimator)\n",
    "   - `dcor_test`: Any dependence detection\n",
    "   - `gpdc_test`: Nonlinear conditional independence\n",
    "\n",
    "### Performance:\n",
    "- Sub-millisecond for small problems\n",
    "- ~100ms for realistic trading applications (5-10 vars, 500 samples)\n",
    "- Parallelized with OpenMP\n",
    "\n",
    "### Use cases:\n",
    "- **Volatility spillover**: Which market's stress predicts another?\n",
    "- **Lead-lag trading**: Exploit predictive relationships\n",
    "- **Risk management**: Understand contagion pathways\n",
    "- **Feature engineering**: Identify causal predictors for ML models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
